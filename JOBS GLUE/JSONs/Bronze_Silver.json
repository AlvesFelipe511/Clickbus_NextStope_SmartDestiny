{
	"jobConfig": {
		"name": "Bronze/Silver",
		"description": "",
		"role": "arn:aws:iam::005102550942:role/service-role/AWSGlueServiceRole-fiap",
		"command": "glueetl",
		"version": "4.0",
		"runtime": null,
		"workerType": "G.1X",
		"numberOfWorkers": 2,
		"maxCapacity": 2,
		"jobRunQueuingEnabled": false,
		"maxRetries": 0,
		"timeout": 2880,
		"maxConcurrentRuns": 1,
		"security": "none",
		"scriptName": "Silver.py",
		"scriptLocation": "s3://aws-glue-assets-005102550942-sa-east-1/scripts/Bronze/",
		"language": "python-3",
		"spark": true,
		"sparkConfiguration": "standard",
		"jobParameters": [],
		"tags": [],
		"jobMode": "DEVELOPER_MODE",
		"createdOn": "2025-09-12T22:52:17.981Z",
		"developerMode": true,
		"connectionsList": [],
		"temporaryDirectory": "s3://aws-glue-assets-005102550942-sa-east-1/temporary/",
		"logging": true,
		"glueHiveMetastore": true,
		"etlAutoTuning": true,
		"metrics": true,
		"observabilityMetrics": true,
		"bookmark": "job-bookmark-disable",
		"sparkPath": "s3://aws-glue-assets-005102550942-sa-east-1/sparkHistoryLogs/",
		"flexExecution": false,
		"minFlexWorkers": null,
		"maintenanceWindow": null
	},
	"hasBeenSaved": false,
	"usageProfileName": null,
	"script": "import sys\r\nfrom awsglue.transforms import *\r\nfrom awsglue.utils import getResolvedOptions\r\nfrom pyspark.context import SparkContext\r\nfrom awsglue.context import GlueContext\r\nfrom awsglue.job import Job\r\n\r\nimport re, time, datetime as _dt, urllib.parse as up\r\nimport boto3\r\nfrom pyspark.sql import functions as F\r\nfrom pyspark.sql.types import StringType, TimestampType, StructType, StructField\r\n\r\n# ------------------ Parâmetros ------------------\r\nARG_KEYS = ['JOB_NAME','bucket','silver_prefix','database','table','use_catalog']\r\ndefaults = {\r\n    'bucket':        'cbchallenge',\r\n    'silver_prefix': 'Silver/Todos os eventos/',  # pasta final única\r\n    'database':      'datalake',\r\n    'table':         'events_silver_unico',\r\n    'use_catalog':   'true'\r\n}\r\ntry:\r\n    args = getResolvedOptions(sys.argv, ARG_KEYS)\r\nexcept Exception:\r\n    args = getResolvedOptions(sys.argv, ['JOB_NAME'])\r\n    for k,v in defaults.items(): args[k] = v\r\n\r\nUSE_CATALOG   = str(args.get('use_catalog','true')).lower() in ('1','true','yes','y')\r\nBUCKET        = args['bucket']\r\nSILVER_PREFIX = args['silver_prefix'].rstrip('/') + '/'   # garante barra\r\nCATALOG_DB    = args['database']\r\nCATALOG_TABLE = args['table']\r\nFINAL_FILENAME= \"events.csv\"  # nome do arquivo único final\r\n\r\n# ------------------ Spark/Glue ------------------\r\n# >>> USAR o contexto já existente no Glue <<<\r\nsc = SparkContext.getOrCreate()\r\nglueContext = GlueContext(sc)\r\nspark = glueContext.spark_session\r\njob = Job(glueContext)\r\njob.init(args['JOB_NAME'], args)\r\n\r\n# ------------------ Funções utilitárias ------------------\r\ndef normalize_cols(df):\r\n    \"\"\"Normaliza cabeçalhos: remove BOM, troca espaços por _, minúsculas.\"\"\"\r\n    out = df\r\n    for c in df.columns:\r\n        new = re.sub(r'\\s+', '_', c.replace('\\ufeff','')).strip().lower()\r\n        if new != c:\r\n            out = out.withColumnRenamed(c, new)\r\n    return out\r\n\r\nfile_col = F.input_file_name()\r\nINGEST_EXPR = F.coalesce(\r\n    F.when(F.length(F.regexp_extract(file_col, r\"date=([0-9]{4}-\\d{2}-\\d{2})\", 1))==0, F.lit(None))\r\n     .otherwise(F.regexp_extract(file_col, r\"date=([0-9]{4}-\\d{2}-\\d{2})\", 1)),\r\n    F.when(F.length(F.regexp_extract(file_col, r\"_(\\d{8})-\\d{6}\\.csv$\", 1))>0,\r\n           F.concat_ws(\"-\", F.substring(F.regexp_extract(file_col, r\"_(\\d{8})-\\d{6}\\.csv$\", 1),1,4),\r\n                           F.substring(F.regexp_extract(file_col, r\"_(\\d{8})-\\d{6}\\.csv$\", 1),5,2),\r\n                           F.substring(F.regexp_extract(file_col, r\"_(\\d{8})-\\d{6}\\.csv$\", 1),7,2)))\r\n     .otherwise(F.lit(None)),\r\n    F.date_format(F.current_timestamp(), \"yyyy-MM-dd\")\r\n)\r\n\r\npt_months = {\"janeiro\":1,\"fevereiro\":2,\"marco\":3,\"março\":3,\"abril\":4,\"maio\":5,\"junho\":6,\r\n             \"julho\":7,\"agosto\":8,\"setembro\":9,\"outubro\":10,\"novembro\":11,\"dezembro\":12,\r\n             \"jan\":1,\"fev\":2,\"mar\":3,\"abr\":4,\"mai\":5,\"jun\":6,\"jul\":7,\"ago\":8,\"set\":9,\"out\":10,\"nov\":11,\"dez\":12}\r\ndef _parse_pt(text):\r\n    if not text: return None\r\n    t = str(text).strip().lower()\r\n    m = re.search(r\"(\\d{1,2})\\s+de\\s+([a-zçãé]+)(?:\\s+de\\s+(\\d{4}))?(?:\\s+às\\s+(\\d{1,2}):(\\d{2}))?\", t)\r\n    if m:\r\n        d=int(m.group(1)); mon=pt_months.get(m.group(2))\r\n        y=int(m.group(3)) if m.group(3) else _dt.date.today().year\r\n        hh=int(m.group(4)) if m.group(4) else 0; mm=int(m.group(5)) if m.group(5) else 0\r\n        if mon:\r\n            try: return _dt.datetime(y,mon,d,hh,mm).strftime(\"%Y-%m-%d %H:%M:%S\")\r\n            except: pass\r\n    m = re.search(r\"(\\d{1,2})\\s+([a-z]{3})\", t)\r\n    if m:\r\n        d=int(m.group(1)); mon=pt_months.get(m.group(2)); y=_dt.date.today().year\r\n        if mon:\r\n            try: return _dt.datetime(y,mon,d).strftime(\"%Y-%m-%d %H:%M:%S\")\r\n            except: pass\r\n    return None\r\nparse_udf = F.udf(_parse_pt, StringType())\r\ndef to_ts_any(c):\r\n    return F.coalesce(\r\n        F.to_timestamp(parse_udf(c)),\r\n        F.to_timestamp(c, \"yyyy-MM-dd HH:mm:ss\"),\r\n        F.to_timestamp(c, \"yyyy-MM-dd\"),\r\n        F.to_timestamp(c, \"dd/MM/yyyy HH:mm\"),\r\n        F.to_timestamp(c, \"dd/MM/yyyy\")\r\n    ).cast(TimestampType())\r\n\r\ndef read_csv(paths):\r\n    dyf = glueContext.create_dynamic_frame.from_options(\r\n        connection_type=\"s3\",\r\n        format=\"csv\",\r\n        format_options={\"quoteChar\": \"\\\"\", \"withHeader\": True, \"separator\": \",\"},\r\n        connection_options={\"paths\": paths, \"recurse\": True}\r\n    )\r\n    return dyf.toDF()\r\n\r\n# Esquema padrão (para DFs vazios, evitando erros no union)\r\nSTD_SCHEMA = StructType([\r\n    StructField(\"event_url\", StringType(), True),\r\n    StructField(\"title\", StringType(), True),\r\n    StructField(\"venue\", StringType(), True),\r\n    StructField(\"city\", StringType(), True),\r\n    StructField(\"event_date_text\", StringType(), True),\r\n    StructField(\"event_date\", TimestampType(), True),\r\n    StructField(\"source\", StringType(), True),\r\n    StructField(\"ingest_date\", StringType(), True),\r\n    StructField(\"src_file\", StringType(), True),\r\n])\r\ndef empty_std_df():\r\n    return spark.createDataFrame([], STD_SCHEMA)\r\n\r\n# ------------------ Paths de origem (maiúsculo e minúsculo) ------------------\r\nsympla_paths  = [f\"s3://{BUCKET}/Bronze/database Sympla/\",  f\"s3://{BUCKET}/bronze/database Sympla/\"]\r\nt360_paths    = [f\"s3://{BUCKET}/Bronze/database Ticket360/\", f\"s3://{BUCKET}/bronze/database Ticket360/\"]\r\neventim_paths = [f\"s3://{BUCKET}/Bronze/database Eventim/\", f\"s3://{BUCKET}/bronze/database Eventim/\",\r\n                 f\"s3://{BUCKET}/Bronze/database Evetim/\", f\"s3://{BUCKET}/bronze/database Evetim/\"]\r\n\r\n# ------------------ Leitura e normalização ------------------\r\nsympla_raw  = read_csv(sympla_paths )\r\nt360_raw    = read_csv(t360_paths   )\r\neventim_raw = read_csv(eventim_paths)\r\n\r\nsympla_df  = normalize_cols(sympla_raw ).withColumn(\"src_file\", file_col)\r\nt360_df    = normalize_cols(t360_raw   ).withColumn(\"src_file\", file_col)\r\neventim_df = normalize_cols(eventim_raw).withColumn(\"src_file\", file_col)\r\n\r\nprint(\"Schemas:\", {\"sympla\": sympla_df.columns, \"ticket360\": t360_df.columns, \"eventim\": eventim_df.columns})\r\n\r\n# ------------------ Mapeamento fixo por fonte (tolerante a vazios) ------------------\r\ndef map_sympla(df):\r\n    cols = set(df.columns)\r\n    req = {\"url\",\"title\",\"place\",\"date\"}\r\n    if not req.issubset(cols):\r\n        print(\"[sympla] vazio/faltando -> DF padrão vazio\")\r\n        return empty_std_df()\r\n    out = (df\r\n        .withColumn(\"event_url\",       F.col(\"url\"))\r\n        .withColumn(\"title\",           F.col(\"title\"))\r\n        .withColumn(\"venue\",           F.col(\"place\"))\r\n        .withColumn(\"city\",            F.lit(None).cast(\"string\"))\r\n        .withColumn(\"event_date_text\", F.col(\"date\"))\r\n        .withColumn(\"event_date\",      to_ts_any(F.col(\"date\")))\r\n        .withColumn(\"source\",          F.lit(\"sympla\"))\r\n        .withColumn(\"ingest_date\",     INGEST_EXPR)\r\n        .select(\"event_url\",\"title\",\"venue\",\"city\",\"event_date_text\",\"event_date\",\"source\",\"ingest_date\",\"src_file\")\r\n    )\r\n    out = out.withColumn(\r\n        \"city\",\r\n        F.when(\r\n            (F.col(\"city\").isNull()) | (F.length(\"city\") == 0),\r\n            F.trim(F.regexp_extract(F.col(\"venue\"), r\"-(?!.*-)\\s*(.*)$\", 1))\r\n        ).otherwise(F.col(\"city\"))\r\n    )\r\n    return out\r\n\r\ndef map_t360(df):\r\n    cols = set(df.columns)\r\n    req = {\"url\",\"title\",\"place\",\"city\",\"date\"}\r\n    if not req.issubset(cols):\r\n        print(\"[ticket360] vazio/faltando -> DF padrão vazio\")\r\n        return empty_std_df()\r\n    return (df\r\n        .withColumn(\"event_url\",       F.col(\"url\"))\r\n        .withColumn(\"title\",           F.col(\"title\"))\r\n        .withColumn(\"venue\",           F.col(\"place\"))\r\n        .withColumn(\"city\",            F.col(\"city\"))\r\n        .withColumn(\"event_date_text\", F.col(\"date\"))\r\n        .withColumn(\"event_date\",      to_ts_any(F.col(\"date\")))\r\n        .withColumn(\"source\",          F.lit(\"ticket360\"))\r\n        .withColumn(\"ingest_date\",     INGEST_EXPR)\r\n        .select(\"event_url\",\"title\",\"venue\",\"city\",\"event_date_text\",\"event_date\",\"source\",\"ingest_date\",\"src_file\")\r\n    )\r\n\r\ndef map_eventim(df):\r\n    cols = set(df.columns)\r\n    req = {\"artist\",\"url\",\"location\",\"date\"}\r\n    if not req.issubset(cols):\r\n        print(\"[eventim] vazio/faltando -> DF padrão vazio\")\r\n        return empty_std_df()\r\n    return (df\r\n        .withColumn(\"event_url\",       F.col(\"url\"))\r\n        .withColumn(\"title\",           F.col(\"artist\"))\r\n        .withColumn(\"venue\",           F.col(\"location\"))\r\n        .withColumn(\"city\",            F.lit(None).cast(\"string\"))\r\n        .withColumn(\"event_date_text\", F.col(\"date\"))\r\n        .withColumn(\"event_date\",      to_ts_any(F.col(\"date\")))\r\n        .withColumn(\"source\",          F.lit(\"eventim\"))\r\n        .withColumn(\"ingest_date\",     INGEST_EXPR)\r\n        .select(\"event_url\",\"title\",\"venue\",\"city\",\"event_date_text\",\"event_date\",\"source\",\"ingest_date\",\"src_file\")\r\n    )\r\n\r\nsympla_norm  = map_sympla(sympla_df)\r\nt360_norm    = map_t360(t360_df)\r\neventim_norm = map_eventim(eventim_df)\r\n\r\n# ------------------ União e deduplicação ------------------\r\nevents_silver = (sympla_norm\r\n                 .unionByName(t360_norm, allowMissingColumns=True)\r\n                 .unionByName(eventim_norm, allowMissingColumns=True)) \\\r\n                .filter(F.col(\"event_url\").isNotNull() & (F.length(\"event_url\")>0)) \\\r\n                .dropDuplicates([\"event_url\"])\r\n\r\nprint(\"Unified rows:\", events_silver.count())\r\n\r\n# ------------------ Gravar ÚNICO CSV em Silver/Todos os eventos/ ------------------\r\ns3 = boto3.client(\"s3\")\r\nFINAL_DIR = SILVER_PREFIX\r\nFINAL_KEY = FINAL_DIR + FINAL_FILENAME\r\nTMP_DIR   = FINAL_DIR + f\"_tmp_write_{int(time.time())}/\"\r\n\r\ncols_out = [\"event_url\",\"title\",\"venue\",\"city\",\"event_date_text\",\"event_date\",\"source\",\"ingest_date\"]\r\n\r\ntotal = events_silver.count()\r\nif total == 0:\r\n    header = \",\".join(cols_out) + \"\\n\"\r\n    s3.put_object(Bucket=BUCKET, Key=FINAL_KEY, Body=header.encode(\"utf-8\"), ContentType=\"text/csv\")\r\n    print(f\"✔ CSV vazio com header em s3://{BUCKET}/{FINAL_KEY}\")\r\nelse:\r\n    (events_silver.select(*cols_out)\r\n        .coalesce(1)\r\n        .write.mode(\"overwrite\")\r\n        .option(\"header\",\"true\")\r\n        .csv(f\"s3://{BUCKET}/{TMP_DIR}\"))\r\n\r\n    resp = s3.list_objects_v2(Bucket=BUCKET, Prefix=TMP_DIR)\r\n    part_key = None\r\n    for o in resp.get(\"Contents\", []):\r\n        k = o[\"Key\"]\r\n        if k.endswith(\".csv\") and \"/_SUCCESS\" not in k:\r\n            part_key = k\r\n            break\r\n    if not part_key:\r\n        raise RuntimeError(\"Não encontrei o arquivo part-*.csv no TMP_DIR\")\r\n\r\n    s3.copy_object(Bucket=BUCKET, Key=FINAL_KEY, CopySource={\"Bucket\": BUCKET, \"Key\": part_key})\r\n    print(f\"✔ CSV único em s3://{BUCKET}/{FINAL_KEY}\")\r\n\r\n    existing = s3.list_objects_v2(Bucket=BUCKET, Prefix=FINAL_DIR)\r\n    for o in existing.get(\"Contents\", []):\r\n        k = o[\"Key\"]\r\n        if k != FINAL_KEY and not k.startswith(TMP_DIR):\r\n            s3.delete_object(Bucket=BUCKET, Key=k)\r\n\r\n    resp2 = s3.list_objects_v2(Bucket=BUCKET, Prefix=TMP_DIR)\r\n    for o in resp2.get(\"Contents\", []):\r\n        s3.delete_object(Bucket=BUCKET, Key=o[\"Key\"])\r\n\r\n# ------------------ (Opcional) Glue Catalog ------------------\r\ndef spark_to_glue_cols(schema):\r\n    out=[]\r\n    for f in schema.fields:\r\n        if f.name == \"src_file\":\r\n            continue\r\n        t = \"string\"\r\n        if str(f.dataType).lower().startswith(\"timestamp\"): t=\"timestamp\"\r\n        out.append({\"Name\": f.name, \"Type\": t})\r\n    return out\r\n\r\nif USE_CATALOG:\r\n    glue = boto3.client(\"glue\")\r\n    try:\r\n        glue.get_database(Name=CATALOG_DB)\r\n    except glue.exceptions.EntityNotFoundException:\r\n        glue.create_database(DatabaseInput={\"Name\": CATALOG_DB, \"Description\": \"DB Silver (arquivo único CSV)\"})\r\n\r\n    cols = spark_to_glue_cols(events_silver.select(*cols_out).schema)\r\n\r\n    table_input = {\r\n        \"Name\": CATALOG_TABLE,\r\n        \"TableType\": \"EXTERNAL_TABLE\",\r\n        \"Parameters\": {\r\n            \"classification\": \"csv\",\r\n            \"skip.header.line.count\": \"1\",\r\n            \"compressionType\": \"none\",\r\n            \"typeOfData\": \"file\"\r\n        },\r\n        \"StorageDescriptor\": {\r\n            \"Columns\": cols,\r\n            \"Location\": f\"s3://{BUCKET}/{FINAL_DIR}\",\r\n            \"InputFormat\": \"org.apache.hadoop.mapred.TextInputFormat\",\r\n            \"OutputFormat\": \"org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\",\r\n            \"SerdeInfo\": {\r\n                \"SerializationLibrary\": \"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\",\r\n                \"Parameters\": { \"field.delim\": \",\", \"escape.delim\": \"\\\\\", \"serialization.format\": \",\" }\r\n            }\r\n        }\r\n    }\r\n\r\n    try:\r\n        glue.get_table(DatabaseName=CATALOG_DB, Name=CATALOG_TABLE)\r\n        glue.update_table(DatabaseName=CATALOG_DB, TableInput=table_input)\r\n        print(f\"✔ Glue Catalog atualizado: {CATALOG_DB}.{CATALOG_TABLE}\")\r\n    except glue.exceptions.EntityNotFoundException:\r\n        glue.create_table(DatabaseName=CATALOG_DB, TableInput=table_input)\r\n        print(f\"✔ Glue Catalog criado: {CATALOG_DB}.{CATALOG_TABLE}\")\r\n\r\n# ------------------ Mover/limpar Bronze por data ------------------\r\ns3 = boto3.client(\"s3\")\r\n\r\ndef to_bucket_key(s3_uri: str):\r\n    p = s3_uri.replace(\"s3://\",\"\"); bkt, key = p.split(\"/\",1); return bkt, key\r\n\r\ndef detect_source_from_key(key: str):\r\n    if \"database Sympla/\" in key: return \"sympla\"\r\n    if \"database Ticket360/\" in key: return \"ticket360\"\r\n    if \"database Eventim/\" in key or \"database Evetim/\" in key: return \"eventim\"\r\n    m = re.search(r\"source=([^/]+)/\", key); return m.group(1) if m else \"desconhecida\"\r\n\r\ndef extract_date_prefix(key: str):\r\n    m = re.search(r\"(.*/date=\\d{4}-\\d{2}-\\d{2}/)\", key)\r\n    return m.group(1) if m else None\r\n\r\ndef extract_date_value(key: str):\r\n    m = re.search(r\"date=(\\d{4}-\\d{2}-\\d{2})\", key)\r\n    return m.group(1) if m else None\r\n\r\ndef processed_key_from_raw(key: str):\r\n    root = key.split('/',1)[0]  # 'Bronze' ou 'bronze'\r\n    src  = detect_source_from_key(key)\r\n    dt   = extract_date_value(key) or \"0000-00-00\"\r\n    fn   = key.split('/')[-1]\r\n    return f\"{root}/Arquivos Processados/source={src}/date={dt}/{fn}\"\r\n\r\ndef list_date_prefixes(bucket, base_prefix):\r\n    dates = set()\r\n    token = None\r\n    while True:\r\n        kw = dict(Bucket=bucket, Prefix=base_prefix)\r\n        if token: kw[\"ContinuationToken\"]=token\r\n        resp = s3.list_objects_v2(**kw)\r\n        for o in resp.get(\"Contents\", []):\r\n            k = o[\"Key\"]\r\n            p = extract_date_prefix(k)\r\n            if p: dates.add(p)\r\n        if not resp.get(\"IsTruncated\"):\r\n            break\r\n        token = resp.get(\"NextContinuationToken\")\r\n    return sorted(dates)\r\n\r\ndef move_all_under_prefix(bucket, date_prefix):\r\n    token = None\r\n    keys = []\r\n    while True:\r\n        kw = dict(Bucket=bucket, Prefix=date_prefix)\r\n        if token: kw[\"ContinuationToken\"]=token\r\n        resp = s3.list_objects_v2(**kw)\r\n        for o in resp.get(\"Contents\", []):\r\n            keys.append(o[\"Key\"])\r\n        if not resp.get(\"IsTruncated\"):\r\n            break\r\n        token = resp.get(\"NextContinuationToken\")\r\n\r\n    moved = 0\r\n    for key in keys:\r\n        if key.endswith(\"/\"):  # marcador de pasta\r\n            continue\r\n        new_key = processed_key_from_raw(key)\r\n        try:\r\n            s3.copy_object(Bucket=bucket, Key=new_key, CopySource={\"Bucket\": bucket, \"Key\": key})\r\n            s3.delete_object(Bucket=bucket, Key=key)\r\n            moved += 1\r\n        except Exception as e:\r\n            print(f\"[WARN] não movido {key}: {e}\")\r\n    print(f\"✔ Movidos {moved} arquivos de s3://{bucket}/{date_prefix} para Arquivos Processados\")\r\n\r\n    try:\r\n        s3.delete_object(Bucket=bucket, Key=date_prefix)\r\n        print(f\"✔ Pasta removida: s3://{bucket}/{date_prefix}\")\r\n    except Exception:\r\n        pass\r\n\r\nSYMPLA_BASES  = [f\"Bronze/database Sympla/\",  f\"bronze/database Sympla/\"]\r\nT360_BASES    = [f\"Bronze/database Ticket360/\", f\"bronze/database Ticket360/\"]\r\nEVENTIM_BASES = [f\"Bronze/database Eventim/\", f\"bronze/database Eventim/\",\r\n                 f\"Bronze/database Evetim/\",  f\"bronze/database Evetim/\"]\r\n\r\ndate_prefixes_seen = set()\r\nfor df in [sympla_df, t360_df, eventim_df]:\r\n    if \"src_file\" in df.columns:\r\n        for uri in [r[0] for r in df.select(\"src_file\").distinct().collect()]:\r\n            if uri and str(uri).startswith(\"s3://\"):\r\n                _, key = to_bucket_key(uri)\r\n                p = extract_date_prefix(key)\r\n                if p: date_prefixes_seen.add(p)\r\n\r\ndef ensure_latest_when_empty(bases, label):\r\n    has_any = any(p for p in date_prefixes_seen if f\"database {label}/\" in p)\r\n    if not has_any:\r\n        all_dates = []\r\n        for base in bases:\r\n            all_dates += list_date_prefixes(BUCKET, base)\r\n        if all_dates:\r\n            latest = sorted(all_dates)[-1]\r\n            date_prefixes_seen.add(latest)\r\n            print(f\"[{label}] sem linhas; adicionada última pasta para mover: {latest}\")\r\n\r\nensure_latest_when_empty(SYMPLA_BASES, \"Sympla\")\r\nensure_latest_when_empty(T360_BASES, \"Ticket360\")\r\nensure_latest_when_empty(EVENTIM_BASES, \"Eventim\")\r\nensure_latest_when_empty(EVENTIM_BASES, \"Evetim\")\r\n\r\nfor date_prefix in sorted(date_prefixes_seen):\r\n    move_all_under_prefix(BUCKET, date_prefix)\r\n\r\n# >>> APENAS UM commit <<<\r\njob.commit()\r\n"
}